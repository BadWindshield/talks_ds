
\documentclass[20pt]{beamer}
\usepackage{amssymb,amsmath}
\usepackage{dsfont}
\usepackage{fancyvrb}  % for including files
\usepackage{empheq}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{hyperref}


\hypersetup{colorlinks=true}

\DeclareMathOperator{\sgn}{sgn}

\geometry{letterpaper, landscape}

\mode<presentation>{\usetheme{Warsaw}}

\title{Bayesian Inference using Markov Chain Monte Carlo (MCMC), Part 2}
\date{November 16, 2021}
\author{William Wong}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}



\begin{frame}{Outline}
\begin{itemize}
\item Review of MCMC

\item PyMC3 examples
  \begin{itemize}
    \item Linear regression
    \item Coal mining disasters
    \item Time series modeling
    \item Mixed media modeling
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Review}
\begin{itemize}
  \item What is Bayesian inference?

Prior assumptions → Look at evidence from data → update prior beliefs → posterior distribution.


  \item The law of total probability $\Pr( A ) = \Pr(A | B_1) \Pr(B_1) + \Pr(A | B_2) \Pr(B_2) + \ldots$

  \item $\theta$: model parameters, $X$: observations
    \begin{align*}
    \Pr( \theta | X ) & = \frac{\Pr( X | \theta) \Pr( \theta )}
      {\Pr( X )} \\
    \Pr( \theta ) & = \text{prior; can make our own choice!} \\
    \Pr( X | \theta ) & = \text{likelihood; fixed by model} \\
    \Pr( \theta | X ) & = \text{posterior} \\
    \Pr( X ) & = \text{evidence; fixed by data}
    \end{align*}

  \item The math gets messy when we rewrite $\Pr(X)$ using the law of total probability.
    \begin{align*}
    \Pr( \theta | X ) & = \frac{\Pr( X | \theta) \Pr( \theta )}
      {\Pr( X )} \\
      & = \frac{\Pr( X | \theta) \Pr( \theta )}
        {\int \Pr(X | \theta) \Pr(\theta) d\theta}
    \end{align*}
  \item When we update our belief, it gets very expensive because we have to compute an integral.
\end{itemize}
\end{frame}

\begin{frame}{MCMC}
\textbf{Question --- how is Markov Chain used?}

\textbf{Question --- how is Monte Carlo used?}

\begin{itemize}
\item \href{https://www.youtube.com/watch?v=0F0QoMCSKJ4}{Stata video, part 1 (00:30 to 4:30)}.
  \begin{itemize}
    \item coin toss. $\theta = \Pr(\text{head})$
    \item $\text{posterior} \propto \text{prior} \bullet \text{likelihood}$. Generally it is not a multiplication but note that we are using a conjugate prior!
    \item $\Pr(\theta | y) \propto \Pr(\theta) \Pr(y | \theta)
= \text{Beta}(1, 1) \bullet \text{Binomial}(4, 10, \theta)$ // 4 heads out of 10 tosses.
  \end{itemize}

\item \href{https://www.youtube.com/watch?v=OTO1DygELpY}{Stata video, part 2 (00:00 to 5:01)}.
  \begin{enumerate}
    \item Markov chain
    \item Monte Carlo
    \item Metropolis-Hastings algorithm
  \end{enumerate}
\end{itemize}

\end{frame}


\begin{frame}{PyMC3}
\begin{itemize}

\item PyMC3 supports Bayesian inference on user-defined probabilistic models.

\item To go over a Jupyter \href{https://github.com/pymc-devs/pymc-examples/blob/main/examples/getting_started.ipynb}{notebook}.

\item Example 1 --- Linear regression where the response function is normally distributed, where the mean is a linear function of two predictor variables, $X_1$ and $X_2$.
\begin{align*}
  Y  &\sim \mathcal{N}(\mu, \sigma^2) \\
  \mu &= \alpha + \beta_1 X_1 + \beta_2 X_2
\end{align*}
Priors
\begin{align*}
  \alpha &\sim \mathcal{N}(0, 100) \\
  \beta_i &\sim \mathcal{N}(0, 100) \\
  \sigma &\sim \lvert\mathcal{N}(0, 1){\rvert}
\end{align*}

\end{itemize}
\end{frame}


\begin{frame}{PyMC3 (con't)}
\begin{itemize}

  \item Example 2 --- Coal mining disasters

In our model, \textbf{the disaster rate $r_t$ changes at some time $s$}.
\begin{align*}
  D_t &\sim \text{Poisson}(r_t), r_t= \begin{cases}
   e, & \text{if } t \le s \\
   l, & \text{if } t > s
   \end{cases} \\
  % D_t &\sim \text{Pois}(r_t) \\
  % r_t & = \begin{cases}
  %  %1\\
  %  %2\\
  %  e, & \text{if } t \le s \\
  %  l, & \text{if } t > s \\
  %  \end{cases}\\
  s &\sim \text{Uniform}(t_l, t_h)\\
  e &\sim \text{exp}(1)\\
  l &\sim \text{exp}(1)
\end{align*}

The parameters are defined as follows:
  \begin{itemize}
   \item $D_t$: The number of disasters in year $t$
   \item $r_t$: The rate parameter of the Poisson distribution of disasters in year $t$.
   \item $s$: The year in which the rate parameter changes (the switchpoint).
   \item $e$: The rate parameter before the switchpoint $s$.
   \item $l$: The rate parameter after the switchpoint $s$.
   \item $t_l$, $t_h$: The lower and upper boundaries of year $t$.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{PyMC3 Time Series Modeling}
Stationarity
\begin{itemize}
  \item The stocastic process $Y_t$ is weakly stationary if
  \begin{itemize}
    \item $\mathbb{E}[Y_t]$ is independent of $t$.
    \item $\text{Var}(Y_t)$ is independent of $t$.
    \item $\text{Covariance}[Y_t]$ is independent of $t$.
  \end{itemize}

  \item \textbf{Question --- what is a stochastic process? How is it related to random variables?}

  \item \textbf{Question --- what are some examples of stationary and non-stationary time series?}
\end{itemize}

The Autocorrelation Function
\begin{itemize}
  \item The autocorrelation of $Y_t$ is $R_{YY}(t_1, t_2) \equiv \mathbb{E}[Y_{t_1} Y_{t_2}]$.

  \item If $Y_t$ is stationary, $R_{YY}(\tau) = \mathbb{E}[Y_{t} Y_{t+\tau}]$, which is independent of $t$.

  \item $R_{YY}(\tau) = R_{YY}(-\tau)$.

  \item $|R_{YY}(\tau)| \le R_{YY}(0)$.

  \item The Fourier transform of $R_{YY}(\tau)$ is the power spectrum of $Y_t$.

  \item \textbf{Question --- what is the algorithmic complexity to compute $R_{YY}(\tau) = \mathbb{E}[Y_{t} Y_{t+\tau}]$?} 

\end{itemize}
\end{frame}

\begin{frame}{PyMC3 Time Series Modeling (con't)}
The AR(1) Model
\begin{itemize}
  \item $Y_t = \theta Y_{t-1} + \epsilon$.

  \item $\epsilon \sim \mathcal{N}(0, \sigma^2)$.

  \item $Y_t$ is weakly stationary if $|\theta| < 1$.

  \item $\mathbb{E}[Y_t] = 0$.

  \item $\text{Var}(Y_t) = \frac{\sigma^2}{1 - \theta^2}$.

  \item $\text{corr}(Y_t, Y_{t-h}) = R_{YY}(h) = \theta^h$.

\end{itemize}
The Jupyter \href{https://github.com/pymc-devs/pymc-examples/blob/main/examples/time_series/AR.ipynb}{notebook}.
\end{frame}

\begin{frame}{Bayesian Mixed Media Modeling (MMM) using PyMC3}
\begin{itemize}
  \item The HelloFresh engineering \href{https://engineering.hellofresh.com/bayesian-media-mix-modeling-using-pymc3-for-fun-and-profit-2bd4667504e6}{blog}

In the model, the number of customers acquired in a week $y_t$ is defined to be
\begin{align*}
  y_t & = \sum_{m=1}^{M} \beta_m f(x_{mt}) + \sum_{c=1}^{C} \beta_c z_{ct} + \epsilon_t \\
  \beta_m & \sim \lvert\mathcal{N}(0, 1){\rvert} \\
  \beta_c & \sim \mathcal{N}(0, 1)
\end{align*}

The parameters are defined as follows:
  \begin{itemize}
   \item $x_{mt}$: The amount of money spent on a marketing channel $m$ in week $t$.
   \item $f()$: A saturation and decay function.
   \item $\beta_m$: The effect of channel $m$ on customer acquisition.
   \item $z_{ct}$: The value of control variable $c$ in week $t$.
   \item $\beta_c$: The effect of control variable $c$.
  \end{itemize}

After building a model, find out how to spend our marketing budget on the various channels by solving a constrained optimization problem.
\end{itemize}
\end{frame}


\end{document}

